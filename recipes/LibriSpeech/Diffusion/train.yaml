# Seed needs to be set at top of yaml, before objects with parameters are made
seed: 1986
__set_seed: !apply:torch.manual_seed [!ref <seed>]
output_folder: !ref results/diff_asr/<seed>
output_wer_folder: !ref <output_folder>/
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt
# pretrain_model_path: !ref GENIE_ckpt-500w
pretrain_model_path: !ref <save_folder>/model.ckpt

generate_path: !ref <output_folder>/samples/
# Data files
data_folder: data/LibriSpeech # e,g./path/to/LibriSpeech
# noise/ris dataset will automatically be downloaded
# data_folder_rirs: !ref <data_folder>
train_splits: ["train-clean-100"]
dev_splits: ["dev-clean"]
test_splits: ["test-clean"]
skip_prep: False
ckpt_interval_minutes: 25 # save checkpoint every N min
train_csv: !ref <output_folder>/train.csv
valid_csv: !ref <output_folder>/dev-clean.csv
test_csv:
   - !ref <output_folder>/test-clean.csv
sorting: ascending
precision: fp32 # bf16, fp16 or fp32
sample_rate: 16000
maxlength: 32
# With data_parallel batch_size is split into N jobs
# With DDP batch_size is multiplied by N jobs
# Must be 3 per GPU to fit 32GB of VRAM
batch_size: 8
test_batch_size: 8
gradient_accumulation_steps: 1
number_of_epochs: 10
# Dataloader options
train_dataloader_opts:
   batch_size: !ref <batch_size>

valid_dataloader_opts:
   batch_size: !ref <batch_size>

test_dataloader_opts:
   batch_size: !ref <test_batch_size>



epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
   limit: !ref <number_of_epochs>

in_channel: 128

# Model Setting
model: !new:model.Diffusion_LM.CrossAttention_Diffusion_LM
  model_channels: 128 
  dropout: 0.1
  in_channels: !ref <in_channel>
  out_channels: 128
  vocab_size: 30522 
  config_name: bert-base-uncased
  logits_mode: 1
  init_pretrained: False
  token_emb_type: random
  encoder_dim: 1024

wavlm_hub: microsoft/wavlm-large
freeze_enc: True
wavlm_folder: !ref <output_folder>/wavlm_checkpoint

enc: !new:speechbrain.lobes.models.huggingface_transformers.wavlm.WavLM
   source: !ref <wavlm_hub>
   output_norm: True
   freeze: !ref <freeze_enc>
   save_path: !ref <wavlm_folder>


model_opt_class: !name:torch.optim.AdamW
   lr: !ref <lr>
   weight_decay: !ref <weight_decay>

enc_opt_class: !name:torch.optim.AdamW
   lr: !ref <lr_enc>
   weight_decay: !ref <weight_decay>

lr_annealing_model: !new:speechbrain.nnet.schedulers.NewBobScheduler
    initial_value: !ref <lr>
    improvement_threshold: 0.0025
    annealing_factor: 0.8
    patient: 0



lr_annealing_enc: !new:speechbrain.nnet.schedulers.NewBobScheduler
    initial_value: !ref <lr_enc>
    improvement_threshold: 0.0025
    annealing_factor: 0.9
    patient: 0

# Masks
padding_mask: !name:speechbrain.lobes.models.transformer.Transformer.get_key_padding_mask

# Diffusion Setting
diff_args:
  steps: 2000
  noise_schedule: sqrt
  rescale_timesteps: True


# how to sample t per batch, uniform is Uniform sampling, loss-second-moment is Sampling according to loss
schedule_sampler: uniform

# Optimizer Setting
lr_anneal_steps: 120000 
warmup_steps: 7200
weight_decay: 0.0
ema_rate: 0.9999
lr: 5e-05
lr_enc: 1e-03
gradient_clipping: -1
eval_interval: 500

num_samples: 1
interval_step : 1

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
    checkpoints_dir: !ref <save_folder>
    recoverables:
        model: !ref <model>
        scheduler_model: !ref <lr_annealing_model>
        scheduler_enc: !ref <lr_annealing_enc>
        counter: !ref <epoch_counter>

pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer
   loadables:
      model: !ref <model>
   paths:
      model: !ref <pretrain_model_path>

modules:
   enc: !ref <enc>
   model: !ref <model>


error_rate_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats

cer_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats
   split_tokens: True

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <train_log>