############################################################################
# Model: SemanticTokenizer + Unit HiFi-GAN
# Tokens: discrete speech units (SemanticTokenizer)
# Training: LibriTTS (English)
# Authors: Pooneh Mousavi
# ############################################################################


###################################
# Experiment Parameters and setup #
###################################
seed: 4321
__set_seed: !apply:torch.manual_seed [!ref <seed>]
output_folder: !ref ./results/semantic_tokenizer/<seed>
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt
progress_sample_path: !ref <output_folder>/samples
epochs: 150
keep_checkpoint_interval: 50
use_tensorboard: False

#################################
# Data files and pre-processing #
#################################
data_folder: data/LibriTTS # e,g./path/to/LibriSpeech

train_split: [train-clean-100]
valid_split: [dev-clean]
test_split: [test-clean]
train_json: !ref <save_folder>/train.json
valid_json: !ref <save_folder>/valid.json
test_json: !ref <save_folder>/test.json
libritts_subsets: null
splits: ["train", "valid", "test"]
split_ratio: null
skip_prep: False


sorting: ascending
dynamic_batching: False
########################################################
#  Encoder  |  HF model                                #
#------------------------------------------------------#
#  HuBERT   |  facebook/hubert-large-ll60k             #
#  Wav2Vec2 |  facebook/wav2vec2-large
#  WavLM    |  overrides                  #
########################################################
ssl_model_type: WavLM # hubert, wavml or wav2vec2
ssl_hub: facebook/wav2vec2-large
ssl_folder: !ref <save_folder>/ssl_checkpoint
freeze_ssl: True
freeze_feature_extractor: True

# Codec Param
n_q: 25
num_codebooks: 1024


# embedding params
vocab_size: 13312  # K-means size
embedding_dim: 1024

ssl_model: !apply:speechbrain.utils.hparams.choice
   value: !ref <ssl_model_type>
   choices:
      wavlm: !new:speechbrain.lobes.models.huggingface_transformers.wavlm.WavLM
         source: !ref <ssl_hub>
         output_norm: False
         freeze: !ref <freeze_ssl>
         freeze_feature_extractor: !ref <freeze_feature_extractor>
         output_all_hiddens: True
         save_path: !ref <ssl_folder>
      hubert: !new:speechbrain.lobes.models.huggingface_transformers.hubert.HuBERT
         source: !ref <ssl_hub>
         output_norm: False
         freeze: !ref <freeze_ssl>
         freeze_feature_extractor: !ref <freeze_feature_extractor>
         output_all_hiddens: True
         save_path: !ref <ssl_folder>
      wav2vec2: !new:speechbrain.lobes.models.huggingface_transformers.wav2vec2.Wav2Vec2
         source: !ref <ssl_hub>
         output_norm: False
         freeze: !ref <freeze_ssl>
         freeze_feature_extractor: !ref <freeze_feature_extractor>
         output_all_hiddens: True
         save_path: !ref <ssl_folder>

################################
# Codec Parameters
################################
codec: !new:speechbrain.lobes.models.discrete.semantic_tokenizer.ResidualVectorQuantizer
        dimension: !ref <embedding_dim>
        n_q: !ref <n_q>
        bins: !ref <num_codebooks>

################################
# Audio Parameters             #
################################
segment: True
segment_size: 8960
code_hop_size: 320
sample_rate: 16000
layer_drop: True

hop_length: 256
win_length: 1024
n_mel_channels: 80
n_fft: 1024
mel_fmin: 0.0
mel_fmax: 8000
mel_normalized: False
power: 1
norm: "slaney"
mel_scale: "slaney"
dynamic_range_compression: True

################################
# Optimization Hyperparameters #
################################
learning_rate: 0.0002
weight_decay: 0.9999
adam_b1: 0.8
adam_b2: 0.99
batch_size: 2

train_dataloader_opts:
  batch_size: !ref <batch_size>
  drop_last: False
  num_workers: 8

valid_dataloader_opts:
  batch_size: 1
  num_workers: 8

test_dataloader_opts:
   batch_size: 1
   num_workers: 8

################################
# Model Parameters and model   #
################################
duration_predictor: False
multi_speaker: False

# generator params
in_channels: !ref <Eembedding_dim>
out_channels: 1

var_pred_hidden_dim: 128
var_pred_kernel_size: 3
var_pred_dropout: 0.5

###########################################################################################################################################################
# version | resblock_type | upsample_kernel_sizes | upsample_factors | resblock_kernel_sizes | upsample_initial_channel | resblock_dilation_sizes
#    1    |      "1"      |      [16,16,4,4]      |    [8, 8, 2, 2]  |     [3, 7, 11]        |           512            | [[1, 3, 5], [1, 3, 5], [1, 3, 5]]
#    2    |      "1"      |      [16,16,4,4]      |    [8, 8, 2, 2]  |     [3, 7, 11]        |           128            | [[1, 3, 5], [1, 3, 5], [1, 3, 5]]
#    3    |      "2"      |       [16,16,8]       |      [8,8,4]     |       [3,5,7]         |           256            |     [[1,2], [2,6], [3,12]]
###########################################################################################################################################################
resblock_type: "1"
resblock_dilation_sizes: [[1, 3, 5], [1, 3, 5], [1, 3, 5]]
resblock_kernel_sizes: [3, 7, 11]
upsample_kernel_sizes: [11, 8, 8, 4, 4]
upsample_initial_channel: 512
upsample_factors: [5, 4, 4, 2, 2]

inference_padding: 5
cond_channels: 0
conv_post_bias: True

mel_spectogram: !name:speechbrain.lobes.models.HifiGAN.mel_spectogram
  sample_rate: !ref <sample_rate>
  hop_length: !ref <hop_length>
  win_length: !ref <win_length>
  n_fft: !ref <n_fft>
  n_mels: !ref <n_mel_channels>
  f_min: !ref <mel_fmin>
  f_max: !ref <mel_fmax>
  power: !ref <power>
  normalized: !ref <mel_normalized>
  norm: !ref <norm>
  mel_scale: !ref <mel_scale>
  compression: !ref <dynamic_range_compression>

generator: !new:speechbrain.lobes.models.HifiGAN.UnitHifiganGenerator
  in_channels: !ref <in_channels>
  out_channels: !ref <out_channels>
  resblock_type: !ref <resblock_type>
  resblock_dilation_sizes: !ref <resblock_dilation_sizes>
  resblock_kernel_sizes: !ref <resblock_kernel_sizes>
  upsample_kernel_sizes: !ref <upsample_kernel_sizes>
  upsample_initial_channel: !ref <upsample_initial_channel>
  upsample_factors: !ref <upsample_factors>
  inference_padding: !ref <inference_padding>
  cond_channels: !ref <cond_channels>
  conv_post_bias: !ref <conv_post_bias>
  vocab_size: !ref <vocab_size>
  embedding_dim: !ref <embedding_dim>
  duration_predictor: !ref <duration_predictor>
  var_pred_hidden_dim: !ref <var_pred_hidden_dim>
  var_pred_kernel_size: !ref <var_pred_kernel_size>
  var_pred_dropout: !ref <var_pred_dropout>
  multi_speaker: !ref <multi_speaker>

discriminator: !new:speechbrain.lobes.models.HifiGAN.HifiganDiscriminator

modules:
  generator: !ref <generator>
  discriminator: !ref <discriminator>
  ssl_model: !ref <ssl_model>
  codec: !ref <codec>

#generator loss
stft_loss: null
mseg_loss: !new:speechbrain.lobes.models.HifiGAN.MSEGLoss
feat_match_loss: !new:speechbrain.lobes.models.HifiGAN.MelganFeatureLoss
l1_spec_loss: !new:speechbrain.lobes.models.HifiGAN.L1SpecLoss
  sample_rate: !ref <sample_rate>
  hop_length: !ref <hop_length>
  win_length: !ref <win_length>
  n_mel_channels: !ref <n_mel_channels>
  n_fft: !ref <n_fft>
  n_stft: !ref <n_fft> // 2 + 1
  mel_fmin: !ref <mel_fmin>
  mel_fmax: null
  mel_normalized: !ref <mel_normalized>
  power: !ref <power>
  dynamic_range_compression: !ref <dynamic_range_compression>
mseg_dur_loss: False

generator_loss: !new:speechbrain.lobes.models.HifiGAN.GeneratorLoss
  stft_loss: !ref <stft_loss>
  stft_loss_weight: 0
  mseg_loss: !ref <mseg_loss>
  mseg_loss_weight: 1
  feat_match_loss: !ref <feat_match_loss>
  feat_match_loss_weight: 10
  l1_spec_loss: !ref  <l1_spec_loss>
  l1_spec_loss_weight: 45
  mseg_dur_loss: !ref <mseg_dur_loss>
  mseg_dur_loss_weight: 1

#discriminator loss
msed_loss: !new:speechbrain.lobes.models.HifiGAN.MSEDLoss

discriminator_loss: !new:speechbrain.lobes.models.HifiGAN.DiscriminatorLoss
  msed_loss: !ref <msed_loss>

#optimizer
opt_class_generator: !name:torch.optim.AdamW
  lr: !ref <learning_rate>
  betas: [!ref <adam_b1>, !ref <adam_b2>]

opt_class_discriminator: !name:torch.optim.AdamW
  lr: !ref <learning_rate>
  betas: [!ref <adam_b1>, !ref <adam_b2>]

sch_class_generator: !name:torch.optim.lr_scheduler.ExponentialLR
  gamma: !ref <weight_decay>
  last_epoch: -1

sch_class_discriminator: !name:torch.optim.lr_scheduler.ExponentialLR
  gamma: !ref <weight_decay>
  last_epoch: -1

#epoch object
epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
  limit: !ref <epochs>

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
  save_file: !ref <train_log>

#checkpointer
checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
  checkpoints_dir: !ref <save_folder>
  recoverables:
    generator: !ref <generator>
    discriminator: !ref <discriminator>
    counter: !ref <epoch_counter>
